"""
AIåˆ†ææœåŠ¡
ä½¿ç”¨å¤§æ¨¡å‹APIè¿›è¡Œå·¥ä½œè®¡åˆ’æ‰§è¡Œæƒ…å†µåˆ†æ
"""
import httpx
import json
from typing import Optional, Dict, List
from datetime import datetime
from sqlalchemy.orm import Session
from app.models.llm_config import LLMConfig
from app.models.task import WeeklyTask
from app.models.user import User


class AIAnalysisService:
    """AIåˆ†ææœåŠ¡ç±»"""

    def __init__(self, db: Session):
        self.db = db

    def get_active_llm_config(self) -> Optional[LLMConfig]:
        """è·å–å½“å‰æ¿€æ´»çš„å¤§æ¨¡å‹é…ç½®"""
        return self.db.query(LLMConfig).filter(
            LLMConfig.is_active == True,
            LLMConfig.is_deleted == False
        ).first()

    async def call_llm_api(self, prompt: str, system_prompt: str = None) -> str:
        """
        è°ƒç”¨å¤§æ¨¡å‹API

        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯

        Returns:
            æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        """
        config = self.get_active_llm_config()
        if not config:
            raise ValueError("æœªé…ç½®å¯ç”¨çš„å¤§æ¨¡å‹")

        # æ„å»ºè¯·æ±‚æ¶ˆæ¯
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        # æ ¹æ®ä¸åŒçš„providerè°ƒç”¨ä¸åŒçš„API
        if config.provider == "deepseek":
            return await self._call_deepseek(config, messages)
        elif config.provider == "openai":
            return await self._call_openai(config, messages)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„provider: {config.provider}")

    async def _call_deepseek(self, config: LLMConfig, messages: List[Dict]) -> str:
        """è°ƒç”¨Deepseek API"""
        url = config.api_base or "https://api.deepseek.com/v1/chat/completions"

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {config.api_key}"
        }

        payload = {
            "model": config.model_name,
            "messages": messages,
            "max_tokens": config.max_tokens,
            "temperature": float(config.temperature),
            "stream": False
        }

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()

            result = response.json()
            return result["choices"][0]["message"]["content"]

    async def _call_openai(self, config: LLMConfig, messages: List[Dict]) -> str:
        """è°ƒç”¨OpenAI API"""
        url = config.api_base or "https://api.openai.com/v1/chat/completions"

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {config.api_key}"
        }

        payload = {
            "model": config.model_name,
            "messages": messages,
            "max_tokens": config.max_tokens,
            "temperature": float(config.temperature)
        }

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()

            result = response.json()
            return result["choices"][0]["message"]["content"]

    def prepare_analysis_data(
        self,
        user_id: Optional[int],
        start_date: str,
        end_date: str
    ) -> Dict:
        """
        å‡†å¤‡åˆ†ææ•°æ®

        Args:
            user_id: ç”¨æˆ·IDï¼ŒNoneè¡¨ç¤ºåˆ†ææ‰€æœ‰ç”¨æˆ·
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            åŒ…å«ä»»åŠ¡æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        # æŸ¥è¯¢ä»»åŠ¡
        query = self.db.query(WeeklyTask).join(User)

        if user_id:
            query = query.filter(WeeklyTask.user_id == user_id)

        # æ ¹æ®æ—¥æœŸç­›é€‰ï¼ˆç®€åŒ–å¤„ç†ï¼Œè¿™é‡Œå¯ä»¥æ”¹è¿›ï¼‰
        tasks = query.all()

        # ç»Ÿè®¡æ•°æ®
        total_tasks = len(tasks)
        completed_tasks = len([t for t in tasks if t.status == "completed"])
        key_tasks = len([t for t in tasks if t.is_key_task])
        key_completed = len([t for t in tasks if t.is_key_task and t.status == "completed"])
        delayed_tasks = len([t for t in tasks if t.status == "delayed"])

        # ä»»åŠ¡è¯¦æƒ…
        task_details = []
        for task in tasks:
            task_details.append({
                "title": task.title,
                "status": task.status,
                "is_key_task": task.is_key_task,
                "week": f"{task.year}å¹´ç¬¬{task.week_number}å‘¨",
                "description": task.description or ""
            })

        return {
            "user_name": tasks[0].user.full_name if tasks and user_id else "å›¢é˜Ÿå…¨ä½“",
            "period": f"{start_date} è‡³ {end_date}",
            "statistics": {
                "total_tasks": total_tasks,
                "completed_tasks": completed_tasks,
                "completion_rate": round(completed_tasks / total_tasks * 100, 1) if total_tasks > 0 else 0,
                "key_tasks": key_tasks,
                "key_completed": key_completed,
                "key_completion_rate": round(key_completed / key_tasks * 100, 1) if key_tasks > 0 else 0,
                "delayed_tasks": delayed_tasks,
                "delay_rate": round(delayed_tasks / total_tasks * 100, 1) if total_tasks > 0 else 0
            },
            "task_details": task_details[:50]  # é™åˆ¶ä»»åŠ¡æ•°é‡ï¼Œé¿å…tokenè¿‡å¤š
        }

    async def analyze_work_performance(
        self,
        user_id: Optional[int],
        start_date: str,
        end_date: str,
        analysis_type: str = "comprehensive"
    ) -> Dict:
        """
        åˆ†æå·¥ä½œç»©æ•ˆ

        Args:
            user_id: ç”¨æˆ·ID
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            analysis_type: åˆ†æç±»å‹

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # å‡†å¤‡æ•°æ®
        data = self.prepare_analysis_data(user_id, start_date, end_date)

        if data["statistics"]["total_tasks"] == 0:
            return {
                "user_name": data["user_name"],
                "analysis_period": data["period"],
                "analysis_result": "è¯¥æ—¶é—´æ®µå†…æ²¡æœ‰ä»»åŠ¡æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚",
                "statistics": data["statistics"],
                "created_at": datetime.now()
            }

        # æ„å»ºæç¤ºè¯
        system_prompt = self._build_system_prompt(analysis_type)
        user_prompt = self._build_user_prompt(data, analysis_type)

        # è°ƒç”¨AI
        try:
            analysis_result = await self.call_llm_api(user_prompt, system_prompt)
        except Exception as e:
            analysis_result = f"AIåˆ†æå¤±è´¥: {str(e)}\n\nåŸºäºæ•°æ®ç»Ÿè®¡ï¼š\n" + self._generate_basic_analysis(data)

        return {
            "user_name": data["user_name"],
            "analysis_period": data["period"],
            "analysis_result": analysis_result,
            "statistics": data["statistics"],
            "created_at": datetime.now()
        }

    def _build_system_prompt(self, analysis_type: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        base_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äººåŠ›èµ„æºç®¡ç†ä¸“å®¶å’Œå·¥ä½œæ•ˆç‡é¡¾é—®ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå‘˜å·¥çš„å‘¨å·¥ä½œè®¡åˆ’æ‰§è¡Œæƒ…å†µï¼Œå¹¶æä¾›ä¸“ä¸šã€å®¢è§‚ã€å»ºè®¾æ€§çš„è¯„ä»·å’Œå»ºè®®ã€‚

åˆ†æè¦æ±‚ï¼š
1. å®¢è§‚å…¬æ­£ï¼šåŸºäºæ•°æ®äº‹å®è¿›è¡Œåˆ†æï¼Œé¿å…ä¸»è§‚è‡†æ–­
2. å…¨é¢æ·±å…¥ï¼šä»å¤šä¸ªç»´åº¦è¿›è¡Œåˆ†æï¼ˆå®Œæˆç‡ã€é‡ç‚¹ä»»åŠ¡ã€å»¶æœŸæƒ…å†µç­‰ï¼‰
3. å»ºè®¾æ€§ï¼šæå‡ºå…·ä½“å¯è¡Œçš„æ”¹è¿›å»ºè®®
4. ç®€æ´æ˜äº†ï¼šä½¿ç”¨æ¸…æ™°çš„ç»“æ„å’Œè¯­è¨€ï¼Œçªå‡ºé‡ç‚¹
"""

        if analysis_type == "performance":
            return base_prompt + "\né‡ç‚¹åˆ†æï¼šå·¥ä½œç»©æ•ˆè¡¨ç°ï¼ŒåŒ…æ‹¬ä»»åŠ¡å®Œæˆè´¨é‡ã€æ•ˆç‡ç­‰ã€‚"
        elif analysis_type == "improvement":
            return base_prompt + "\né‡ç‚¹åˆ†æï¼šå­˜åœ¨çš„é—®é¢˜å’Œæ”¹è¿›ç©ºé—´ï¼Œæä¾›è¯¦ç»†çš„æ”¹è¿›å»ºè®®ã€‚"
        else:
            return base_prompt + "\nè¿›è¡Œå…¨é¢ç»¼åˆåˆ†æã€‚"

    def _build_user_prompt(self, data: Dict, analysis_type: str) -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯"""
        stats = data["statistics"]

        prompt = f"""è¯·åˆ†æä»¥ä¸‹å‘˜å·¥çš„å·¥ä½œè®¡åˆ’æ‰§è¡Œæƒ…å†µï¼š

**å‘˜å·¥ä¿¡æ¯**ï¼š{data["user_name"]}
**åˆ†æå‘¨æœŸ**ï¼š{data["period"]}

**ç»Ÿè®¡æ•°æ®**ï¼š
- æ€»ä»»åŠ¡æ•°ï¼š{stats["total_tasks"]}
- å·²å®Œæˆï¼š{stats["completed_tasks"]} ({stats["completion_rate"]}%)
- é‡ç‚¹ä»»åŠ¡ï¼š{stats["key_tasks"]} (å·²å®Œæˆ{stats["key_completed"]}ä¸ªï¼Œå®Œæˆç‡{stats["key_completion_rate"]}%)
- å»¶æœŸä»»åŠ¡ï¼š{stats["delayed_tasks"]} (å»¶æœŸç‡{stats["delay_rate"]}%)

**ä»»åŠ¡è¯¦æƒ…**ï¼ˆéƒ¨åˆ†å±•ç¤ºï¼‰ï¼š
"""

        # æ·»åŠ ä»»åŠ¡è¯¦æƒ…
        for i, task in enumerate(data["task_details"][:20], 1):
            key_mark = "ã€é‡ç‚¹ã€‘" if task["is_key_task"] else ""
            status_map = {
                "completed": "âœ…å·²å®Œæˆ",
                "in_progress": "ğŸ”„è¿›è¡Œä¸­",
                "todo": "ğŸ“‹å¾…åŠ",
                "delayed": "âš ï¸å·²å»¶æœŸ"
            }
            status = status_map.get(task["status"], task["status"])
            prompt += f"\n{i}. {key_mark}{task['title']} - {status} ({task['week']})"

        if len(data["task_details"]) > 20:
            prompt += f"\n... (å…±{len(data['task_details'])}ä¸ªä»»åŠ¡)"

        prompt += "\n\nè¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æï¼š\n"
        prompt += "1. **å·¥ä½œå®Œæˆæƒ…å†µ**ï¼šæ•´ä½“å®Œæˆç‡ã€é‡ç‚¹ä»»åŠ¡å®Œæˆæƒ…å†µ\n"
        prompt += "2. **å·¥ä½œè´¨é‡è¯„ä¼°**ï¼šä»»åŠ¡å»¶æœŸæƒ…å†µåˆ†æ\n"
        prompt += "3. **ä¼˜ç‚¹ä¸äº®ç‚¹**ï¼šè¡¨ç°çªå‡ºçš„æ–¹é¢\n"
        prompt += "4. **é—®é¢˜ä¸ä¸è¶³**ï¼šéœ€è¦æ”¹è¿›çš„åœ°æ–¹\n"
        prompt += "5. **æ”¹è¿›å»ºè®®**ï¼šå…·ä½“å¯è¡Œçš„æ”¹è¿›æªæ–½\n"
        prompt += "6. **ç»¼åˆè¯„ä»·**ï¼šæ€»ä½“è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰å’Œæ€»ç»“\n\n"
        prompt += "è¯·ä½¿ç”¨markdownæ ¼å¼ï¼Œç»“æ„æ¸…æ™°ï¼Œé‡ç‚¹çªå‡ºã€‚"

        return prompt

    def _generate_basic_analysis(self, data: Dict) -> str:
        """ç”ŸæˆåŸºç¡€åˆ†æï¼ˆå½“AIè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        stats = data["statistics"]

        analysis = f"""### åŸºç¡€æ•°æ®åˆ†æ

**æ•´ä½“å®Œæˆæƒ…å†µ**
- ä»»åŠ¡å®Œæˆç‡ï¼š{stats["completion_rate"]}%
- é‡ç‚¹ä»»åŠ¡å®Œæˆç‡ï¼š{stats["key_completion_rate"]}%
- ä»»åŠ¡å»¶æœŸç‡ï¼š{stats["delay_rate"]}%

**è¯„ä¼°ç»“æœ**
"""

        if stats["completion_rate"] >= 80:
            analysis += "- âœ… æ•´ä½“å®Œæˆç‡è‰¯å¥½ï¼Œå·¥ä½œæ‰§è¡ŒåŠ›è¾ƒå¼º\n"
        elif stats["completion_rate"] >= 60:
            analysis += "- âš ï¸ æ•´ä½“å®Œæˆç‡ä¸­ç­‰ï¼Œæœ‰å¾…æå‡\n"
        else:
            analysis += "- âŒ æ•´ä½“å®Œæˆç‡åä½ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›\n"

        if stats["key_completion_rate"] >= 80:
            analysis += "- âœ… é‡ç‚¹ä»»åŠ¡æŠŠæ¡å‡†ç¡®ï¼Œä¼˜å…ˆçº§ç®¡ç†è‰¯å¥½\n"
        else:
            analysis += "- âš ï¸ é‡ç‚¹ä»»åŠ¡å®Œæˆç‡ä¸è¶³ï¼Œå»ºè®®åŠ å¼ºé‡ç‚¹å·¥ä½œçš„æ¨è¿›\n"

        if stats["delay_rate"] > 20:
            analysis += "- âš ï¸ å»¶æœŸç‡è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–æ—¶é—´ç®¡ç†å’Œä»»åŠ¡è§„åˆ’\n"

        return analysis
